
import tensorflow as tf
import os
from data_loader import load_datasets
from model import build_cnn2
from model import vgg19
from model import efficientNetB0
from model import build_cnn3
from model import build_cnn4
from model import build_cnn5
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from utils import plot_and_save


TRAIN_DIR = r'/app/train'
VAL_DIR = r'/app/validation'
MODEL_SAVE_PATH = r'/app/saved_model/cifar10_model100.keras'
OUTPUT_DIR = r'/app/output'

if __name__ == "__main__":
    if tf.config.list_physical_devices('GPU'):
        print("GPU í™œì„±í™” í™•ì¸ë¨")
    else:
        print("CPU ì‚¬ìš©")

    train_ds, val_ds, class_names = load_datasets(TRAIN_DIR, VAL_DIR)

    # ==================================================================
    # ğŸš€ 1ë‹¨ê³„: íŠ¹ì„± ì¶”ì¶œ (Feature Extraction)
    # ==================================================================
    print("--- 1ë‹¨ê³„: íŠ¹ì„± ì¶”ì¶œ(ëª¸í†µì€ ê³ ì •) ì‹œì‘ ---")
    model = efficientNetB0()  # model.pyì—ì„œ ef_base.trainable = False ìƒíƒœ

    model.compile(optimizer=Adam(learning_rate=1e-3),
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                  metrics=['accuracy'])

    # 1ë‹¨ê³„ EarlyStopping: ë¹„êµì  ì—¬ìœ ë¡­ê²Œ ì„¤ì •
    early_stopping_1 = EarlyStopping(
        monitor='val_loss',
        patience=10,  # ì¶©ë¶„í•œ í•™ìŠµì„ ìœ„í•´ patience ì¦ê°€
        verbose=1,
        restore_best_weights=True
    )

    history = model.fit(train_ds,
                        epochs=50,  # ì–´ì°¨í”¼ ì¡°ê¸° ì¢…ë£Œë¨
                        validation_data=val_ds,
                        callbacks=[early_stopping_1])

    print("\n--- 1ë‹¨ê³„ ì™„ë£Œ. ìµœì  val_accuracy: {:.4f} ---".format(max(history.history['val_accuracy'])))

    # ==================================================================
    # ğŸš€ 2ë‹¨ê³„: ë¯¸ì„¸ ì¡°ì • (Fine-Tuning)
    # ==================================================================
    print("\n--- 2ë‹¨ê³„: ë¯¸ì„¸ ì¡°ì •(ëª¸í†µ ì „ì²´ í•™ìŠµ) ì‹œì‘ ---")

    # 1. Base ëª¨ë¸ì˜ ë™ê²°ì„ í•´ì œí•©ë‹ˆë‹¤.
    # model.summary()ë¥¼ ë³´ë©´ ef_baseëŠ” ë‘ ë²ˆì§¸ ë ˆì´ì–´ì…ë‹ˆë‹¤. (ì²« ë²ˆì§¸ëŠ” Resizing)
    model.layers[1].trainable = True
    print("EfficientNetB0 Base ëª¨ë¸ì˜ ë™ê²°ì„ í•´ì œí–ˆìŠµë‹ˆë‹¤.")

    # 2. â—â—â— ì•„ì£¼ ë‚®ì€ í•™ìŠµë¥ ë¡œ ë‹¤ì‹œ ì»´íŒŒì¼í•©ë‹ˆë‹¤. (ê°€ì¥ ì¤‘ìš”) â—â—â—
    model.compile(optimizer=Adam(learning_rate=1e-5),  # 0.00001
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                  metrics=['accuracy'])

    print("ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ (1e-5)ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ ì»´íŒŒì¼í–ˆìŠµë‹ˆë‹¤.")
    model.summary()

    # 2ë‹¨ê³„ EarlyStopping: ë” ë¯¼ê°í•˜ê²Œ ì„¤ì •í•˜ì—¬ ê³¼ì í•© ë°©ì§€
    early_stopping_2 = EarlyStopping(
        monitor='val_loss',
        patience=5,  # ê³¼ì í•©ì´ ì‹œì‘ë˜ë©´ ë¹ ë¥´ê²Œ ì¤‘ë‹¨
        verbose=1,
        restore_best_weights=True
    )

    # 3. ì¶”ê°€ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
    # ì´ì „ í•™ìŠµì´ ëë‚œ epochì—ì„œ ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.
    fine_tune_epochs = 30  # ì¶”ê°€ í•™ìŠµ ì—í­
    total_epochs = len(history.epoch) + fine_tune_epochs

    history_fine = model.fit(train_ds,
                             epochs=total_epochs,
                             initial_epoch=len(history.epoch),  # ì´ì „ í•™ìŠµ ì§€ì ë¶€í„° ì‹œì‘
                             validation_data=val_ds,
                             callbacks=[early_stopping_2])

    model.save(MODEL_SAVE_PATH)
    plot_and_save(history_fine, OUTPUT_DIR)
    print("\n--- ëª¨ë“  í•™ìŠµ ì™„ë£Œ. ìµœì¢… ëª¨ë¸ì´ {}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ---".format(MODEL_SAVE_PATH))
